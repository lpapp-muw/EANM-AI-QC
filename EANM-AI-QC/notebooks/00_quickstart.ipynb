{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2fc29f",
   "metadata": {},
   "source": [
    "# EANM-AI-QC — Quickstart (Jupyter)\n",
    "\n",
    "This notebook runs the CLI scripts and then visualizes the produced CSV artifacts under `Results/`.\n",
    "\n",
    "Key point: the notebook kernel must use the same Python environment as your terminal `.venv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4534c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "REPO = Path().resolve()\n",
    "assert (REPO/'qnm_qai.py').exists(), \"Run Jupyter from the repository root (folder containing qnm_qai.py)\"\n",
    "print(\"Repo root:\", REPO)\n",
    "print(\"Python:\", sys.executable)\n",
    "\n",
    "# Best-effort: ensure Results exists\n",
    "(Path(\"Results\")).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_matplotlib()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    return float(num) / float(den) if float(den) != 0.0 else float(\"nan\")\n",
    "\n",
    "def cm_metrics_from_preds(y_true, prob1, threshold=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    prob1 = np.asarray(prob1, dtype=float)\n",
    "    y_pred = (prob1 >= float(threshold)).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv  = _safe_div(tp, tp + fp)\n",
    "    npv  = _safe_div(tn, tn + fn)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # AUC is undefined if only one class is present\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true, prob1)\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bal),\n",
    "        \"auc\": float(auc),\n",
    "    }, cm\n",
    "\n",
    "def show_confusion_matrix(cm, title=\"Confusion matrix\", labels=(\"0\", \"1\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    cm = np.asarray(cm, dtype=int)\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 3.6))\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0, 1], labels=labels)\n",
    "    ax.set_yticks([0, 1], labels=labels)\n",
    "\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_roc_curve(y_true, prob1, title=\"ROC curve\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"ROC: only one class present in y_true; skipping.\")\n",
    "        return\n",
    "    RocCurveDisplay.from_predictions(y_true, prob1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def ensure_matplotlib():\n",
    "    try:\n",
    "        import matplotlib.pyplot as _plt  # noqa: F401\n",
    "    except Exception:\n",
    "        # Notebook-safe install\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbd197",
   "metadata": {},
   "source": [
    "## Run the demo (no SHAP/LIME)\n",
    "\n",
    "This trains models and writes metrics + predictions to `Results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash examples/run_all_examples.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89963f32",
   "metadata": {},
   "source": [
    "## Summary tables (`Results/*__results.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "for csv in sorted(Path(\"Results\").glob(\"*__results.csv\")):\n",
    "    print(\"\\n\", csv)\n",
    "    display(pd.read_csv(csv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b64446",
   "metadata": {},
   "source": [
    "## Confusion matrices (test set) per method\n",
    "\n",
    "This reads `Results/<dataset>/<method>/predictions/test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbeb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "datasets = [p for p in Path(\"Results\").iterdir() if p.is_dir()]\n",
    "for ds in sorted(datasets):\n",
    "    print(\"\\n=== DATASET:\", ds.name, \"===\")\n",
    "    for method_dir in sorted(ds.iterdir()):\n",
    "        pred_path = method_dir / \"predictions\" / \"test.csv\"\n",
    "        if not pred_path.exists():\n",
    "            continue\n",
    "        dfp = pd.read_csv(pred_path)\n",
    "        if \"true_label\" not in dfp.columns:\n",
    "            print(method_dir.name, \"test.csv has no true_label; skipping confusion matrix.\")\n",
    "            continue\n",
    "        y = dfp[\"true_label\"].astype(int).to_numpy()\n",
    "        prob1 = dfp[\"prob_1\"].astype(float).to_numpy()\n",
    "\n",
    "        metrics, cm = cm_metrics_from_preds(y, prob1, threshold=0.5)\n",
    "        print(\"\\n\", method_dir.name, {k: metrics[k] for k in [\"accuracy\",\"auc\",\"balanced_accuracy\",\"sensitivity\",\"specificity\"]})\n",
    "        show_confusion_matrix(cm, title=f\"{ds.name} / {method_dir.name} — test CM (thr=0.5)\")\n",
    "        show_roc_curve(y, prob1, title=f\"{ds.name} / {method_dir.name} — test ROC\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
