{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600b6046",
   "metadata": {},
   "source": [
    "# Tabular radiomics demo\n",
    "\n",
    "Trains/evaluates:\n",
    "- `pl_kernel_svm`\n",
    "- `pl_qcnn_alt`\n",
    "- `pl_qcnn_muw`\n",
    "\n",
    "Then visualizes **test-set** confusion matrices and ROC curves from the stored prediction CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f665e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "REPO = Path().resolve()\n",
    "assert (REPO/'qnm_qai.py').exists(), \"Run Jupyter from the repository root (folder containing qnm_qai.py)\"\n",
    "print(\"Repo root:\", REPO)\n",
    "print(\"Python:\", sys.executable)\n",
    "\n",
    "# Best-effort: ensure Results exists\n",
    "(Path(\"Results\")).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_matplotlib()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    return float(num) / float(den) if float(den) != 0.0 else float(\"nan\")\n",
    "\n",
    "def cm_metrics_from_preds(y_true, prob1, threshold=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    prob1 = np.asarray(prob1, dtype=float)\n",
    "    y_pred = (prob1 >= float(threshold)).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv  = _safe_div(tp, tp + fp)\n",
    "    npv  = _safe_div(tn, tn + fn)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # AUC is undefined if only one class is present\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true, prob1)\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bal),\n",
    "        \"auc\": float(auc),\n",
    "    }, cm\n",
    "\n",
    "def show_confusion_matrix(cm, title=\"Confusion matrix\", labels=(\"0\", \"1\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    cm = np.asarray(cm, dtype=int)\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 3.6))\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0, 1], labels=labels)\n",
    "    ax.set_yticks([0, 1], labels=labels)\n",
    "\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_roc_curve(y_true, prob1, title=\"ROC curve\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"ROC: only one class present in y_true; skipping.\")\n",
    "        return\n",
    "    RocCurveDisplay.from_predictions(y_true, prob1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def ensure_matplotlib():\n",
    "    try:\n",
    "        import matplotlib.pyplot as _plt  # noqa: F401\n",
    "    except Exception:\n",
    "        # Notebook-safe install\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff011063",
   "metadata": {},
   "source": [
    "## Build tabular CSVs from raw FDB/LDB (demo_data/tabular/raw)\n",
    "\n",
    "Produces:\n",
    "- `demo_data/tabular/real_train.csv`\n",
    "- `demo_data/tabular/real_infer.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python examples/build_tabular_from_fdb_ldb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ce364",
   "metadata": {},
   "source": [
    "## Train + evaluate (no SHAP/LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python qnm_qai.py run   --input demo_data/tabular/real_train.csv   --infer demo_data/tabular/real_infer.csv   --input-type tabular   --methods pl_kernel_svm,pl_qcnn_alt,pl_qcnn_muw   --results-dir Results   --test-size 0.25   --max-samples-per-method 80   --qcnn-epochs 15   --qcnn-lr 0.02   --qcnn-batch-size 16   --qcnn-init-scale 0.1   --seed 0   --no-explain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571c52a",
   "metadata": {},
   "source": [
    "## Summary metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "summary = Path(\"Results\")/\"real_train__results.csv\"\n",
    "df = pd.read_csv(summary)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec2d65",
   "metadata": {},
   "source": [
    "## Per-method test confusion matrices + ROC\n",
    "\n",
    "Uses `Results/real_train/<method>/predictions/test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79120d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base = Path(\"Results\")/\"real_train\"\n",
    "for method in [\"pl_kernel_svm\",\"pl_qcnn_alt\",\"pl_qcnn_muw\"]:\n",
    "    pred_path = base/method/\"predictions\"/\"test.csv\"\n",
    "    print(\"\\nMETHOD:\", method)\n",
    "    dfp = pd.read_csv(pred_path)\n",
    "    display(dfp.head(10))\n",
    "\n",
    "    y = dfp[\"true_label\"].astype(int).to_numpy()\n",
    "    prob1 = dfp[\"prob_1\"].astype(float).to_numpy()\n",
    "\n",
    "    metrics, cm = cm_metrics_from_preds(y, prob1, threshold=0.5)\n",
    "    display(pd.DataFrame([metrics]))\n",
    "    show_confusion_matrix(cm, title=f\"real_train / {method} — test CM (thr=0.5)\")\n",
    "    show_roc_curve(y, prob1, title=f\"real_train / {method} — test ROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b7216",
   "metadata": {},
   "source": [
    "## If the run stored an optimized decision threshold\n",
    "\n",
    "Some configurations store a non-0.5 threshold in metadata to maximize balanced accuracy on the TRAIN split. This cell prints it if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"Results\")/\"real_train\"\n",
    "for method in [\"pl_kernel_svm\",\"pl_qcnn_alt\",\"pl_qcnn_muw\"]:\n",
    "    meta = base/method/\"model\"/\"metadata.json\"\n",
    "    if not meta.exists():\n",
    "        continue\n",
    "    j = json.loads(meta.read_text())\n",
    "    thr = j.get(\"decision_threshold\", None)\n",
    "    print(method, \"decision_threshold:\", thr)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
