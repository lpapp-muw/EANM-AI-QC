{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac28cfa8",
   "metadata": {},
   "source": [
    "# Results dashboard\n",
    "\n",
    "Scans `Results/` and produces:\n",
    "- nicely formatted summary tables\n",
    "- confusion matrices + ROC curves (test)\n",
    "- SHAP/LIME plots when explanation outputs exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "REPO = Path().resolve()\n",
    "assert (REPO/'qnm_qai.py').exists(), \"Run Jupyter from the repository root (folder containing qnm_qai.py)\"\n",
    "print(\"Repo root:\", REPO)\n",
    "print(\"Python:\", sys.executable)\n",
    "\n",
    "# Best-effort: ensure Results exists\n",
    "(Path(\"Results\")).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54692dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_matplotlib()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    return float(num) / float(den) if float(den) != 0.0 else float(\"nan\")\n",
    "\n",
    "def cm_metrics_from_preds(y_true, prob1, threshold=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    prob1 = np.asarray(prob1, dtype=float)\n",
    "    y_pred = (prob1 >= float(threshold)).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv  = _safe_div(tp, tp + fp)\n",
    "    npv  = _safe_div(tn, tn + fn)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # AUC is undefined if only one class is present\n",
    "    auc = float(\"nan\")\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true, prob1)\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bal),\n",
    "        \"auc\": float(auc),\n",
    "    }, cm\n",
    "\n",
    "def show_confusion_matrix(cm, title=\"Confusion matrix\", labels=(\"0\", \"1\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    cm = np.asarray(cm, dtype=int)\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 3.6))\n",
    "    im = ax.imshow(cm)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0, 1], labels=labels)\n",
    "    ax.set_yticks([0, 1], labels=labels)\n",
    "\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_roc_curve(y_true, prob1, title=\"ROC curve\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(\"ROC: only one class present in y_true; skipping.\")\n",
    "        return\n",
    "    RocCurveDisplay.from_predictions(y_true, prob1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def ensure_matplotlib():\n",
    "    try:\n",
    "        import matplotlib.pyplot as _plt  # noqa: F401\n",
    "    except Exception:\n",
    "        # Notebook-safe install\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25314d2",
   "metadata": {},
   "source": [
    "## Load all summary result CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "summaries = sorted(Path(\"Results\").glob(\"*__results.csv\"))\n",
    "print(\"Summary CSVs:\", len(summaries))\n",
    "for p in summaries:\n",
    "    print(\" -\", p)\n",
    "\n",
    "dfs = []\n",
    "for p in summaries:\n",
    "    df = pd.read_csv(p)\n",
    "    df.insert(0, \"dataset\", p.name.replace(\"__results.csv\",\"\"))\n",
    "    dfs.append(df)\n",
    "\n",
    "if dfs:\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    display(all_df)\n",
    "else:\n",
    "    print(\"No summary CSVs found. Run: bash examples/run_all_examples.sh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052eea2",
   "metadata": {},
   "source": [
    "## Confusion matrices for every method (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "for dataset_dir in sorted([p for p in Path(\"Results\").iterdir() if p.is_dir()]):\n",
    "    print(\"\\n=== DATASET:\", dataset_dir.name, \"===\")\n",
    "    for method_dir in sorted([p for p in dataset_dir.iterdir() if p.is_dir()]):\n",
    "        pred_path = method_dir/\"predictions\"/\"test.csv\"\n",
    "        if not pred_path.exists():\n",
    "            continue\n",
    "        dfp = pd.read_csv(pred_path)\n",
    "        if \"true_label\" not in dfp.columns:\n",
    "            continue\n",
    "\n",
    "        y = dfp[\"true_label\"].astype(int).to_numpy()\n",
    "        prob1 = dfp[\"prob_1\"].astype(float).to_numpy()\n",
    "\n",
    "        metrics, cm = cm_metrics_from_preds(y, prob1, threshold=0.5)\n",
    "        print(method_dir.name, {k: metrics[k] for k in [\"accuracy\",\"auc\",\"balanced_accuracy\",\"sensitivity\",\"specificity\"]})\n",
    "        show_confusion_matrix(cm, title=f\"{dataset_dir.name} / {method_dir.name} — test CM (thr=0.5)\")\n",
    "        show_roc_curve(y, prob1, title=f\"{dataset_dir.name} / {method_dir.name} — test ROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d2f3e",
   "metadata": {},
   "source": [
    "## SHAP + LIME plots (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def plot_shap_summary(shap_csv: Path, top=15, title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = pd.read_csv(shap_csv)\n",
    "    cand_cols = [c for c in df.columns if c.lower() in (\"mean_abs_shap\",\"mean_abs\",\"abs_mean\",\"mean_abs_value\")]\n",
    "    col = cand_cols[0] if cand_cols else df.columns[-1]\n",
    "    df = df.sort_values(col, ascending=False).head(int(top)).iloc[::-1]\n",
    "    plt.figure(figsize=(6.5, 4.5))\n",
    "    plt.barh(df[\"feature\"], df[col])\n",
    "    plt.title(title or f\"SHAP summary — top {top}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_lime_for_sample(lime_csv: Path, sample_id=None, top=12, title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = pd.read_csv(lime_csv)\n",
    "    if \"sample_id\" not in df.columns:\n",
    "        for c in df.columns:\n",
    "            if \"sample\" in c.lower() and \"id\" in c.lower():\n",
    "                df = df.rename(columns={c: \"sample_id\"})\n",
    "                break\n",
    "    if sample_id is None:\n",
    "        sample_id = df[\"sample_id\"].iloc[0]\n",
    "    d = df[df[\"sample_id\"] == sample_id].copy()\n",
    "    wcol = \"weight\" if \"weight\" in d.columns else d.columns[-1]\n",
    "    d[\"absw\"] = d[wcol].abs()\n",
    "    d = d.sort_values(\"absw\", ascending=False).head(int(top)).sort_values(wcol, ascending=True)\n",
    "    plt.figure(figsize=(6.5, 4.5))\n",
    "    plt.barh(d[\"feature\"], d[wcol])\n",
    "    plt.title(title or f\"LIME — sample {sample_id}\")\n",
    "    plt.xlabel(wcol)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for dataset_dir in sorted([p for p in Path(\"Results\").iterdir() if p.is_dir()]):\n",
    "    for method_dir in sorted([p for p in dataset_dir.iterdir() if p.is_dir()]):\n",
    "        shap_csv = method_dir/\"explain\"/\"shap\"/\"shap_summary.csv\"\n",
    "        lime_csv = method_dir/\"explain\"/\"lime\"/\"lime_explanations.csv\"\n",
    "        if shap_csv.exists():\n",
    "            print(\"\\nSHAP:\", dataset_dir.name, method_dir.name)\n",
    "            display(pd.read_csv(shap_csv).head(10))\n",
    "            plot_shap_summary(shap_csv, top=15, title=f\"{dataset_dir.name}/{method_dir.name}\")\n",
    "        if lime_csv.exists():\n",
    "            print(\"\\nLIME:\", dataset_dir.name, method_dir.name)\n",
    "            display(pd.read_csv(lime_csv).head(10))\n",
    "            plot_lime_for_sample(lime_csv, sample_id=None, top=12, title=f\"{dataset_dir.name}/{method_dir.name}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
